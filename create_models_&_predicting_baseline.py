# -*- coding: utf-8 -*-
"""Create Models & Predicting Baseline

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yPAoQ8iYcwhWjuAdOZmf2bAwJHy3idIo
"""

# Copyright 2019 Google Inc.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""# Fine-Tuning Reddit Data with BERT"""

from sklearn.model_selection import train_test_split
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
from datetime import datetime

!pip install bert-tensorflow

import bert
from bert import run_classifier
from bert import optimization
from bert import tokenization

"""Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.

Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.

Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist).
"""

# Set the output directory for saving model file
# Optionally, set a GCP bucket location

OUTPUT_DIR = 'reddit-model-masked'#@param {type:"string"}
#@markdown Whether or not to clear/delete the directory and create a new one
DO_DELETE = False #@param {type:"boolean"}
#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.
USE_BUCKET = True #@param {type:"boolean"}
BUCKET = 'redbert' #@param {type:"string"}

if USE_BUCKET:
  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)
  from google.colab import auth
  auth.authenticate_user()

if DO_DELETE:
  try:
    tf.gfile.DeleteRecursively(OUTPUT_DIR)
  except:
    # Doesn't matter if the directory didn't exist
    pass
tf.gfile.MakeDirs(OUTPUT_DIR)
print('***** Model output directory: {} *****'.format(OUTPUT_DIR))

"""#Data"""

from google.colab import auth
auth.authenticate_user()
!mkdir /tmp/reddit-data
!gsutil cp gs://redbert/reddit-data/ground_truth_lines.txt /tmp/reddit-data
!gsutil cp gs://redbert/reddit-data/reddit_random_lines.txt /tmp/reddit-data
!gsutil cp gs://redbert/reddit-data/l2_lines.txt /tmp/reddit-data
!gsutil cp gs://redbert/reddit-data/l2_ne_lines.txt /tmp/reddit-data
!gsutil cp gs://redbert/reddit-data/l1samplelines_masked.txt /tmp/reddit-data
!gsutil cp gs://redbert/reddit-data/l1samplelines_topten_masked.txt /tmp/reddit-data
!gsutil cp gs://redbert/reddit-data/l2_ne_lines.txt /tmp/reddit-data
!gsutil cp gs://redbert/reddit-masked/*.txt /tmp/reddit-data

def load_stripped_lines(filename):
  with open(filename) as f:
    return pd.DataFrame([s.strip() for s in f.readlines()])

gtdf = load_stripped_lines("/tmp/reddit-data/ground_truth_lines.txt")
l2df = load_stripped_lines("/tmp/reddit-data/l2_lines.txt")
rrdf = load_stripped_lines("/tmp/reddit-data/reddit_random_lines.txt")
l2nedf = load_stripped_lines("/tmp/reddit-data/l2_ne_lines.txt")

!gsutil cp gs://redbert/reddit-data/l1english*.csv /tmp/reddit-data

mask_gtdf = load_stripped_lines("/tmp/reddit-data/ground_truth_masked.txt")
mask_l2df = load_stripped_lines("/tmp/reddit-data/l2_lines_masked.txt")
mask_l2nedf = load_stripped_lines("/tmp/reddit-data/l2_ne_masked.txt")
mask_rrdf = load_stripped_lines("/tmp/reddit-data/reddit_random_masked.txt")

seed = 44

def clip_and_relabel(data, length, label):
  balanced = data.sample(n=length, random_state=44).copy()
  
  if len(balanced.columns) == 1:
    balanced.columns = ['sentence']
  elif len(balanced.columns) == 2:
    balanced.columns = ['sentence', 'label']
  
  balanced['label'] = label
  return balanced
  
rr_balanced = clip_and_relabel(rrdf, len(gtdf), 0)
gt_balanced = clip_and_relabel(gtdf, len(gtdf), 1)
l2_balanced = clip_and_relabel(l2df, len(gtdf), 0)

rrmask_balanced = clip_and_relabel(mask_rrdf, len(gtdf), 0)
gtmask_balanced = clip_and_relabel(mask_gtdf, len(gtdf), 1)
l2mask_balanced = clip_and_relabel(mask_l2df, len(gtdf), 0)

l2ne_balanced = clip_and_relabel(l2nedf, len(gtdf), 0)
l2nemask_balanced = clip_and_relabel(mask_l2nedf, len(gtdf), 0)

l2_balanced.to_json("l2_dataframe.json")

gt_balanced.to_json("gt_dataframe.json")

# These two were generated offline in order to avoid reuploading too many GB
l1df = pd.read_csv("/tmp/reddit-data/l1english_sample.csv")
l1nedf = pd.read_csv("/tmp/reddit-data/l1english_topten_sample.csv")

mask_l1nedf = load_stripped_lines("/tmp/reddit-data/l1samplelines_topten_masked.txt")
mask_l1df = load_stripped_lines("/tmp/reddit-data/l1samplelines_masked.txt")

l1mask_balanced = clip_and_relabel(mask_l1df, len(gtdf), 0)
l1nemask_balanced = clip_and_relabel(mask_l1nedf, len(gtdf), 0)

"""Now that we have our data we can split it into test and train datasets."""

#@markdown Whether or not to clear/delete the directory and create a new one
TRAIN_MODE = 'Masked Ground Truth vs Masked Random Comments' #@param ["Ground Truth vs Random Comments", "Masked Ground Truth vs Masked Random Comments"]

supported_modes = {
    'Ground Truth vs Random Comments': rr_balanced.append(gt_balanced),
    'Masked Ground Truth vs Masked Random Comments': rrmask_balanced.append(gtmask_balanced),
}

# Option 1: Ground Truth vs Reddit Random
if TRAIN_MODE in supported_modes:
  train, test = train_test_split(supported_modes[TRAIN_MODE], test_size=0.20)
  print(f"Created training dataset for {TRAIN_MODE}")
  print("Train dataset breakdown:")
  display(train.groupby('label').count())
  print("Test dataset breakdown:")
  display(test.groupby('label').count())
else:
  print("Mode not yet supported!")

"""For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"""

DATA_COLUMN = 'sentence'
LABEL_COLUMN = 'label'
# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'
label_list = [0, 1]

"""#Data Preprocessing
We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.

- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. 
- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.
- `label` is the label for our example, i.e. True, False
"""

# Use the InputExample class from BERT's run_classifier code to create examples from the data
train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example
                                                                   text_a = x[DATA_COLUMN], 
                                                                   text_b = None, 
                                                                   label = x[LABEL_COLUMN]), axis = 1)

test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, 
                                                                   text_a = x[DATA_COLUMN], 
                                                                   text_b = None, 
                                                                   label = x[LABEL_COLUMN]), axis = 1)

"""Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):


1. Lowercase our text (if we're using a BERT lowercase model)
2. Tokenize it (i.e. "sally says hi" -> ["sally", "says", "hi"])
3. Break words into WordPieces (i.e. "calling" -> ["call", "##ing"])
4. Map our words to indexes using a vocab file that BERT provides
5. Add special "CLS" and "SEP" tokens (see the [readme](https://github.com/google-research/bert))
6. Append "index" and "segment" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))

Happily, we don't have to worry about most of these details.

To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:
"""

# This is a path to an uncased (all lowercase) version of BERT
BERT_MODEL_HUB = "https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1"

def create_tokenizer_from_hub_module():
  """Get the vocab file and casing info from the Hub module."""
  with tf.Graph().as_default():
    bert_module = hub.Module(BERT_MODEL_HUB)
    tokenization_info = bert_module(signature="tokenization_info", as_dict=True)
    with tf.Session() as sess:
      vocab_file, do_lower_case = sess.run([tokenization_info["vocab_file"],
                                            tokenization_info["do_lower_case"]])
      
  return bert.tokenization.FullTokenizer(
      vocab_file=vocab_file, do_lower_case=do_lower_case)

tokenizer = create_tokenizer_from_hub_module()

"""Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info["do_lower_case"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:

Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands.
"""

# We'll set sequences to be at most 128 tokens long.
MAX_SEQ_LENGTH = 128
# Convert our train and test features to InputFeatures that BERT understands.
train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)
test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)

"""#Creating a model

Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning).
"""

def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,
                 num_labels):
  """Creates a classification model."""

  bert_module = hub.Module(
      BERT_MODEL_HUB,
      trainable=True)
  bert_inputs = dict(
      input_ids=input_ids,
      input_mask=input_mask,
      segment_ids=segment_ids)
  bert_outputs = bert_module(
      inputs=bert_inputs,
      signature="tokens",
      as_dict=True)

  # Use "pooled_output" for classification tasks on an entire sentence.
  # Use "sequence_outputs" for token-level output.
  output_layer = bert_outputs["pooled_output"]

  hidden_size = output_layer.shape[-1].value

  # Create our own layer to tune for politeness data.
  output_weights = tf.get_variable(
      "output_weights", [num_labels, hidden_size],
      initializer=tf.truncated_normal_initializer(stddev=0.02))

  output_bias = tf.get_variable(
      "output_bias", [num_labels], initializer=tf.zeros_initializer())

  with tf.variable_scope("loss"):

    # Dropout helps prevent overfitting
    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)

    logits = tf.matmul(output_layer, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    log_probs = tf.nn.log_softmax(logits, axis=-1)

    # Convert labels into one-hot encoding
    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)

    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))
    # If we're predicting, we want predicted labels and the probabiltiies.
    if is_predicting:
      return (predicted_labels, log_probs)

    # If we're train/eval, compute loss between predicted and actual label
    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)
    loss = tf.reduce_mean(per_example_loss)
    return (loss, predicted_labels, log_probs)

"""Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."""

# model_fn_builder actually creates our model function
# using the passed parameters for num_labels, learning_rate, etc.
def model_fn_builder(num_labels, learning_rate, num_train_steps,
                     num_warmup_steps):
  """Returns `model_fn` closure for TPUEstimator."""
  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
    """The `model_fn` for TPUEstimator."""

    input_ids = features["input_ids"]
    input_mask = features["input_mask"]
    segment_ids = features["segment_ids"]
    label_ids = features["label_ids"]

    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)
    
    # TRAIN and EVAL
    if not is_predicting:

      (loss, predicted_labels, log_probs) = create_model(
        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)

      train_op = bert.optimization.create_optimizer(
          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)

      # Calculate evaluation metrics. 
      def metric_fn(label_ids, predicted_labels):
        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)
        f1_score = tf.contrib.metrics.f1_score(
            label_ids,
            predicted_labels)
        auc = tf.metrics.auc(
            label_ids,
            predicted_labels)
        recall = tf.metrics.recall(
            label_ids,
            predicted_labels)
        precision = tf.metrics.precision(
            label_ids,
            predicted_labels) 
        true_pos = tf.metrics.true_positives(
            label_ids,
            predicted_labels)
        true_neg = tf.metrics.true_negatives(
            label_ids,
            predicted_labels)   
        false_pos = tf.metrics.false_positives(
            label_ids,
            predicted_labels)  
        false_neg = tf.metrics.false_negatives(
            label_ids,
            predicted_labels)
        return {
            "eval_accuracy": accuracy,
            "f1_score": f1_score,
            "auc": auc,
            "precision": precision,
            "recall": recall,
            "true_positives": true_pos,
            "true_negatives": true_neg,
            "false_positives": false_pos,
            "false_negatives": false_neg
        }

      eval_metrics = metric_fn(label_ids, predicted_labels)

      if mode == tf.estimator.ModeKeys.TRAIN:
        return tf.estimator.EstimatorSpec(mode=mode,
          loss=loss,
          train_op=train_op)
      else:
          return tf.estimator.EstimatorSpec(mode=mode,
            loss=loss,
            eval_metric_ops=eval_metrics)
    else:
      (predicted_labels, log_probs) = create_model(
        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)

      predictions = {
          'probabilities': log_probs,
          'labels': predicted_labels
      }
      return tf.estimator.EstimatorSpec(mode, predictions=predictions)

  # Return the actual model function in the closure
  return model_fn

# Compute train and warmup steps from batch size
# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)
BATCH_SIZE = 32
LEARNING_RATE = 2e-5
NUM_TRAIN_EPOCHS = 3.0
# Warmup is a period of time where hte learning rate 
# is small and gradually increases--usually helps training.
WARMUP_PROPORTION = 0.1
# Model configs
SAVE_CHECKPOINTS_STEPS = 500
SAVE_SUMMARY_STEPS = 100

# Compute # train and warmup steps from batch size
num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)
num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)

# Specify outpit directory and number of checkpoint steps to save
run_config = tf.estimator.RunConfig(
    model_dir=OUTPUT_DIR,
    save_summary_steps=SAVE_SUMMARY_STEPS,
    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)

model_fn = model_fn_builder(
  num_labels=len(label_list),
  learning_rate=LEARNING_RATE,
  num_train_steps=num_train_steps,
  num_warmup_steps=num_warmup_steps)

estimator = tf.estimator.Estimator(
  model_fn=model_fn,
  config=run_config,
  params={"batch_size": BATCH_SIZE})

"""Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."""

# Create an input function for training. drop_remainder = True for using TPUs.
train_input_fn = bert.run_classifier.input_fn_builder(
    features=train_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=True,
    drop_remainder=False)

"""Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."""

print(f'Beginning Training!')
current_time = datetime.now()
estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)
print("Training took time ", datetime.now() - current_time)

"""Now let's use our test data to see how well our model did:"""

test_input_fn = run_classifier.input_fn_builder(
    features=test_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=False,
    drop_remainder=False)

estimator.evaluate(input_fn=test_input_fn, steps=None)

l2_relabelled = l2_balanced
l2_relabelled['label'] = 0

l2mask_relabelled = l2mask_balanced
l2mask_relabelled['label'] = 0

l2ne_relabelled = l2ne_balanced
l2ne_relabelled['label'] = 0

l2nemask_relabelled = l2nemask_balanced
l2nemask_relabelled['label'] = 0

l1nemask_relabelled = l1nemask_balanced
l1nemask_relabelled['label'] = 0

l1mask_relabelled = l1mask_balanced
l1mask_relabelled['label'] = 0

l2mask_InputExamples = l2mask_relabelled.apply(lambda x: bert.run_classifier.InputExample(guid=None, 
                                                                   text_a = x[DATA_COLUMN], 
                                                                   text_b = None, 
                                                                   label = x[LABEL_COLUMN]), axis = 1)

l2mask_features = bert.run_classifier.convert_examples_to_features(l2mask_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)

l2_InputExamples = l2_relabelled.apply(lambda x: bert.run_classifier.InputExample(guid=None, 
                                                                   text_a = x[DATA_COLUMN], 
                                                                   text_b = None, 
                                                                   label = x[LABEL_COLUMN]), axis = 1)

l2_features = bert.run_classifier.convert_examples_to_features(l2_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)

l2nemask_InputExamples = l2nemask_relabelled.apply(lambda x: bert.run_classifier.InputExample(guid=None, 
                                                                   text_a = x[DATA_COLUMN], 
                                                                   text_b = None, 
                                                                   label = x[LABEL_COLUMN]), axis = 1)

l2nemask_features = bert.run_classifier.convert_examples_to_features(l2nemask_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)

l2ne_InputExamples = l2ne_relabelled.apply(lambda x: bert.run_classifier.InputExample(guid=None, 
                                                                   text_a = x[DATA_COLUMN], 
                                                                   text_b = None, 
                                                                   label = x[LABEL_COLUMN]), axis = 1)

l2ne_features = bert.run_classifier.convert_examples_to_features(l2ne_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)

l1_InputExamples = l1df.apply(lambda x: bert.run_classifier.InputExample(guid=None, 
                                                                   text_a = x[DATA_COLUMN], 
                                                                   text_b = None, 
                                                                   label = x[LABEL_COLUMN]), axis = 1)

l1_features = bert.run_classifier.convert_examples_to_features(l1_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)

l1ne_InputExamples = l1nedf.apply(lambda x: bert.run_classifier.InputExample(guid=None, 
                                                                   text_a = x[DATA_COLUMN], 
                                                                   text_b = None, 
                                                                   label = x[LABEL_COLUMN]), axis = 1)

l1ne_features = bert.run_classifier.convert_examples_to_features(l1ne_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)

l1mask_InputExamples = l1mask_relabelled.apply(lambda x: bert.run_classifier.InputExample(guid=None, 
                                                                   text_a = x[DATA_COLUMN], 
                                                                   text_b = None, 
                                                                   label = x[LABEL_COLUMN]), axis = 1)

l1mask_features = bert.run_classifier.convert_examples_to_features(l1mask_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)

l1nemask_InputExamples = l1nemask_relabelled.apply(lambda x: bert.run_classifier.InputExample(guid=None, 
                                                                   text_a = x[DATA_COLUMN], 
                                                                   text_b = None, 
                                                                   label = x[LABEL_COLUMN]), axis = 1)

l1nemask_features = bert.run_classifier.convert_examples_to_features(l1nemask_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)

# Evaluate misclassifications on random L2 comments.
l2_test_input_fn = run_classifier.input_fn_builder(
    features=l2_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=False,
    drop_remainder=False)

l2mask_test_input_fn = run_classifier.input_fn_builder(
    features=l2mask_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=False,
    drop_remainder=False)

l2ne_test_input_fn = run_classifier.input_fn_builder(
    features=l2ne_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=False,
    drop_remainder=False)

l2nemask_test_input_fn = run_classifier.input_fn_builder(
    features=l2nemask_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=False,
    drop_remainder=False)

# Evaluate misclassifications on random L1 English comments

l1_test_input_fn = run_classifier.input_fn_builder(
    features=l1_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=False,
    drop_remainder=False)

l1ne_test_input_fn = run_classifier.input_fn_builder(
    features=l1ne_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=False,
    drop_remainder=False)

l1mask_test_input_fn = run_classifier.input_fn_builder(
    features=l1mask_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=False,
    drop_remainder=False)

l1nemask_test_input_fn = run_classifier.input_fn_builder(
    features=l1nemask_features,
    seq_length=MAX_SEQ_LENGTH,
    is_training=False,
    drop_remainder=False)

estimator.evaluate(input_fn=l1_test_input_fn, steps=None)

estimator.evaluate(input_fn=l1ne_test_input_fn, steps=None)

estimator.evaluate(input_fn=l1mask_test_input_fn, steps=None)

estimator.evaluate(input_fn=l1nemask_test_input_fn, steps=None)

estimator.evaluate(input_fn=l2_test_input_fn, steps=None)

estimator.evaluate(input_fn=l2mask_test_input_fn, steps=None)

estimator.evaluate(input_fn=l2ne_test_input_fn, steps=None)

estimator.evaluate(input_fn=l2nemask_test_input_fn, steps=None)

"""Now let's write code to make predictions on new sentences:"""

def getPrediction(in_sentences):
  labels = [0, 1]
  input_examples = [run_classifier.InputExample(guid="", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, "" is just a dummy label
  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)
  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)
  predictions = estimator.predict(predict_input_fn)
  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]

pred_sentences = [
  "Hillary won the popular vote",
  "Sam won the popular vote",
  "Cats won the popular vote",
  "Hillary played the popular vote",
  "Wow I hope that’s not me",
  "Cats Wow I hope that’s not me",
  "Hillary is 68, she just can’t be all healthy",
  "Hillary is 68, he just can’t be all healthy",
  "Hillary is 68, she just can’t be all there",
  "Hillary is 68, he just can’t be all there"
]

predictions = getPrediction(pred_sentences)

